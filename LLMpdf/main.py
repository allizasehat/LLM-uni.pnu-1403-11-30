import streamlit as st
import fitz  # PyMuPDF Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ PDF
import chromadb
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
import ollama

# ØªÙ†Ø¸ÛŒÙ… ØµÙØ­Ù‡
st.set_page_config(page_title="PDF Chatbot", layout="wide")


# ØªØ§Ø¨Ø¹ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ø§Ø² PDF
def extract_text_from_pdf(pdf_file):
    doc = fitz.open(stream=pdf_file.read(), filetype="pdf")
    text = ""
    for page in doc:
        text += page.get_text("text") + "\n"
    return text


# ØªØ§Ø¨Ø¹ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡ Ø¯Ø± ChromaDB
def process_and_store_text(text):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    chunks = text_splitter.split_text(text)

    client = chromadb.PersistentClient(path="./chroma_db")  # Ø°Ø®ÛŒØ±Ù‡ Ù¾Ø§ÛŒØ¯Ø§Ø±
    collection = client.get_or_create_collection(name="pdf_data")

    # Ù†ÙˆØ§Ø± Ù¾ÛŒØ´Ø±ÙØª
    progress_bar = st.progress(0)

    for i, chunk in enumerate(chunks):
        collection.add(documents=[chunk], ids=[f"chunk_{i}"])

        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¯Ø±ØµØ¯ Ù¾ÛŒØ´Ø±ÙØª
        progress = (i + 1) / len(chunks) * 100
        progress_bar.progress(int(progress))

    return collection


# ØªØ§Ø¨Ø¹ Ø¬Ø³ØªØ¬Ùˆ Ùˆ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ù…ØªÙ†
def search_db(query):
    client = chromadb.PersistentClient(path="./chroma_db")
    collection = client.get_collection(name="pdf_data")

    results = collection.query(query_texts=[query], n_results=3)
    return [doc for doc in results["documents"][0]]


# Ø±Ø§Ø¨Ø· Ú©Ø§Ø±Ø¨Ø±ÛŒ Streamlit
st.title("ğŸ“„ PDF Chatbot with Ollama")

# Ù„ÛŒØ³Øª Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ú†Øª
if 'messages' not in st.session_state:
    st.session_state['messages'] = []

uploaded_file = st.file_uploader("ÛŒÚ© ÙØ§ÛŒÙ„ PDF Ø¢Ù¾Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯:", type=["pdf"])

if uploaded_file is not None:
    st.success("ÙØ§ÛŒÙ„ Ø¢Ù¾Ù„ÙˆØ¯ Ø´Ø¯! Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´...")
    text = extract_text_from_pdf(uploaded_file)
    process_and_store_text(text)
    st.success("PDF Ù¾Ø±Ø¯Ø§Ø²Ø´ Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯. Ø­Ø§Ù„Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø³ÙˆØ§Ù„ Ø¨Ù¾Ø±Ø³ÛŒØ¯.")

    query = st.text_input("Ø³ÙˆØ§Ù„ Ø®ÙˆØ¯ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯:")
    if query:
        # Ø°Ø®ÛŒØ±Ù‡ Ù¾Ø±Ø³Ø´ Ú©Ø§Ø±Ø¨Ø± Ø¯Ø± ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ú†Øª
        st.session_state['messages'].append({"role": "user", "content": query})

        # Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ùˆ Ú¯Ø±ÙØªÙ† Ù…ØªÙ† Ù…Ø±ØªØ¨Ø·
        retrieved_texts = search_db(query)
        context = " ".join(retrieved_texts)

        # Ù†Ù…Ø§ÛŒØ´ Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ
        st.write("### Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ø§Ø±Ø³Ø§Ù„â€ŒØ´Ø¯Ù‡ Ø¨Ù‡ Ù…Ø¯Ù„:")
        st.write(st.session_state['messages'])

        # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„ Ù…Ø­Ù„ÛŒ phi3:mini Ø¨Ø§ Ø´Ù†Ø§Ø³Ù‡ Ù…Ù†Ø§Ø³Ø¨
        model_id = "phi3:mini"  # Ø§Ú¯Ø± Ù…Ø¯Ù„ Ù…Ø­Ù„ÛŒ Ø¨Ø§Ø´Ø¯ØŒ Ù†ÛŒØ§Ø²ÛŒ Ø¨Ù‡ Ù…Ø³ÛŒØ± Ù†ÛŒØ³Øª.

        try:
            # Ø§Ø±Ø³Ø§Ù„ ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ú†Øª Ø¨Ù‡ Ù…Ø¯Ù„
            response = ollama.chat(model=model_id, messages=st.session_state['messages'] + [
                {"role": "system", "content": "Ù¾Ø§Ø³Ø® Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…ØªÙ† Ø²ÛŒØ± Ø¨Ø¯Ù‡:\n" + context}
            ])

            # Ù†Ù…Ø§ÛŒØ´ Ù¾Ø§Ø³Ø® Ù…Ø¯Ù„
            st.write("### Ù¾Ø§Ø³Ø® Ù…Ø¯Ù„:")
            st.write(response["message"]["content"])

            # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù¾Ø§Ø³Ø® Ù…Ø¯Ù„ Ø¨Ù‡ ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ú†Øª
            st.session_state['messages'].append({"role": "assistant", "content": response["message"]["content"]})
        except Exception as e:
            st.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§ Ù…Ø¯Ù„: {e}")

        # Ù†Ù…Ø§ÛŒØ´ ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ú†Øª
        st.write("### ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ú†Øª:")
        for message in st.session_state['messages']:
            if message['role'] == 'user':
                st.markdown(f"**Ø´Ù…Ø§:** {message['content']}")
            else:
                st.markdown(f"**Ù…Ø¯Ù„:** {message['content']}")
